# 亿级流量系统多级缓存架构5

## 资源隔离服务降级

### 什么叫资源隔离？

服务隔离是指在一个大型系统中，可以把原连接在一起的组件，模块，服务，资源拆分开。

那么在系统出现故障的时候，可以隔离故障，阻止传播，不会出现滚雪球和雪崩的效应。

隔离的方式主要有

- 线程隔离
- 进程隔离
- 集群隔离
- 机房隔离
- 读写隔离
- 动静隔离
- 爬虫隔离
- 等....



#### 线程隔离

主要是在多线程环境下，对线程池进行治理，把核心业务和非核心业务分割开。

但是在多线程池下，不同线程池中的线程

**在使用Netty的时候**

netty本身是负载网络io的框架，想要做到网络服务和业务隔离， 首先它不应该和项目中的业务逻辑在同一线程池中。
netty在做多线程项目的时候，少量的netty线程负责接受大量请求，请求会分发给业务线程池，而业务线程池处理不了那么多，这就要在他们之间放一个缓冲区，这就是Servlet 3.0

如果同一线程池中



**Tomcat**中的资源隔离

##### servlet3请求隔离

servlet 2.0
用户 -> Connector,和配置http1.1(BIO), apr, nio, nio2.0 -> 创建业务线程Servlet，执行计算，结果返回Connector，Connector把结果返回给用户。 对于每一个用户，Connector都会创建一个业务线程，
所以要设置一个连接池。Servlet在Tomcat里是单例的，所以他没有状态，但这不重要，session、header和cookie等状态信息放在线程里，threadLocal做资源隔离，线程执行完任务就从ThreadLocal里面清除掉（<= Tomcat
 6）.最大的问题是：Servlet和Connector可以处理的请求数量差别很大。NIO的问题在Connector就解决了。BIO是来一个用户就开一个和用户保持网络连接的线程，这些线程就在那里一边等着，一边再去开启业务线程
 APR方式不是和应用线程连接，而是和系统内核连接，连进来之后再通知应用，要不要开业务线程。  
 
异步的时候NIO可以接受20000个线程，但是后面再开业务线程开不了那么多，<=6.0时能做的就是限制连接数。划分连接和业务两组线程的目的是提高qps，即介入连接数。还有一个叫tps，transaction，是要跑通整个
业务的。请求进来之后，在业务缓冲区中等待超时，可以给一个稍微有好一点的反馈：这次请求失败，请稍后再请求。这样能提高qps的个数，但不能提高tps的个数。  

按业务模块划分，把不同的业务分到不同的为服务上去执行，一个业务再怎么OOM也不会影响其他的模块。出问题的时候尽可能的不互相影响，这是做资源隔离的目的之一


![1571047024970](images\1571047024970.png)

需要Tomcat7以上版本

1个线程就可以接线程，接到之后放到一个缓冲区（业务队列），然后再由业务线程池去处理，或者决定开多少个线程，这样就达到了连接响应的隔离，这样可以让连接数接得更多。
将来跨JVM，在集群里就是消息中间件的应用，保证消息不丢

Tomcat把连接介入和业务处理拆分成两个线程池来处理，即

![1571047174645](images\1571047174645.png)

**Connector**  介入连接，配置socket处理方式

可选

- Http11Protocol	
- Http11NioProtocol	
- Http11Nio2Protocol	
- Http11AprProtocol

Connector**

在异                                                                                                                                                                      介入连接，可以采用BI/O，nio，ajp,apr.

性能提现的是qps，即介入连接数



**Executor** **Servlet**

可以使用独立的线程池来维护servlet的创建。



**那么**

connector能介入的请求肯定比业务复杂的servlet处理的个数要多，在中间，Tomcat还加入了队列，来等待servlet线程池空闲。



这两步是Tomcat内核完成的，在一阶段无法区分具体业务或资源，所以只能在连接介入，servlet初始化完成后我们根据自己的业务线去划分独立的连接池。

那么在独立的业务或资源中如果出现崩溃，不会影响其他的业务线程，从而达到资源隔离和服务降级的效果。



在使用了servlet3之后，系统线程隔离变得更灵活了。可以划分核心业务队列和非核心业务队列，





##### 但是

1. 资源一旦出现问题，虽然是隔离状态，想要让资源重新可用，很难做到不重启jvm。
2. 线程池内部线程如果出现OOM、FullGC、cpu耗尽等问题也是无法控制的. 无法通过设置线程个数的方式精确调节内存的使用，除非coder自己心里真的很有数

##### **结论**

线程隔离，只能保证在分配线程这个资源上进行隔离，并不能保证整体稳定性



#### **进程隔离**

在项目最初期，一般都是allinone的技术架构，然后做负载均衡，session共享。

通过线程隔离无法完全避免雪崩。

java  cpu、内存这些资源可以通过不同的虚拟机进程来做隔离。

1. 集群式计算 - 服务的复制（就像Redis主从复制），一变多，通过负载均衡器，如nginx、lvs、haproxy 分发请求。但是这个项目还是AllinOne的，这样不能达到资源隔离的效果。这是可以让LB长个脑子，访问user的时候
               把请求分发给某些机器，访问item的时候分发给另一些机器，代码都是完整的只是另外那一部分不运行就行了，一台机器挂了，对于整个集群不影响，甚至只对于处理user业务的那一台机器再做一层LB，提高
               可用性。这都需要Lua脚本在nginx上实现，分析URI。这一点lvs就做不到了。缺点是太大的时候修改很复杂，启动消耗时间长，加载模块和类时间长。只更新一丢丢可以用代码热部署，热部署的东西太多，
              必须重启了
2. 分布式计算 - 拆分微服务，某一组服务器只放user，另一组只放item, 如果不够用的话可以针对单独的某个服务去增加机器，项目变大的时候也无所谓，因为已经模块化了。服务之间的通信可以用Kafka了，也可以把Kafka放在
              LB和服务之间，这样既可以做到数据上的缓冲也能做到计算和流量上的缓冲了。在nginx中发现Kafka消息被消费了，然后返回response解开socket阻塞的状态。详细：nginx通过Lua脚本把消息放在Kafka里，
              然后被service消费，消费完了之后，再写回Kafka里（另一个topic），Kafka再通知nginx上的Lua，然后再向客户端返回response。Lua可以做Kafka的生产者和消费者。划分模块、划分服务是做架构师避
              不开的工作。拆模块、设计数据库表很难一次成功，推倒重来的几率很高。敏捷开发不能做到所有功能都去不断迭代，他能做到快速交付，他不能解决整体架构设计上的不合理之处，架子已经搭好了。想一次搭好、
              后面不再改是很难的。做架构师就是不断的去思考如何划分微服务
              
以上两种怎么选？看需求、进度、资金、允许你干多少年（假设允许干20年，想怎么玩就怎么玩，大不了干19年最后一年辞职，如果下周就上线，那就选第一种集群计算）。看钱和时间。。。  
服务内部的隔离：有这么一个框架叫Hystrix，他可以通过线程和信号量进行隔离

#### 集群隔离

如果系统中某个业务模块包含像

- 抢购、秒杀
- 存储I/O密集度高
- 网络I/o高
- 计算I/O高

这类需求的时候，很容易在并发量高的时候因为这种功能把整个模块占有的资源全部耗尽，导致响应编码甚至节点不可用。

##### 解决方案

- 独立拆分模块
- 微服务化

可以使用hystrix在微服务中隔离分布式服务故障。他可以通过线程和信号量进行隔离。

#### 机房隔离异地多活

解决数据容量大，计算，i/o（网络）密集度高的问题

把服务建立整体副本（计算服务、数据存储），在多机房内做异地多活或冷备份、是微服务数据异构的放大版
通过LB向多个不同的异地多活的机房做负载，当在机房层面出现问题的时候，可以通过智能dns、httpdns、负载均衡等技术快速切换  
httpdns只适用于CS（Client到Server，Client可以是桌面或手机app或者自己写的浏览器，不能是浏览器。）类的客户端。httpdns不是给web浏览器的用户解析域名的，而是给app应用程序的用户来解析域名的。流程是这样的：
app先访问比如www.mashibing.com/upload，接口是可以访问就去访问，否则app发一个请求给httpdns接口，httpdns接口把可用的IP返回给app，客户端app可以再重试一次。httpdns服务器必须得可用性极高，否则httpdns
挂了之后，app就只能用本地缓存的dns了。  

而浏览器就不能这样了，他有一套固定的套路：先找自己的host配置，然后找浏览器缓存的dns，然后去连接dns服务器。。。总不能把所有用户的浏览器都黑了吧。  

#### 数据（读写）分离
往一台机器上写了数据之后会同步到其他机器，如果有异地多活，要拉专线以提高同步数据的速度。网络分区不一定是由于脑裂，数据的网络同步延迟也算
通过主从模式，将mysql、redis等数据存储服务集群化，读写分离，那么在写入数据不可用的时候，也可以通过**重试机制**临时通过其他节点读取到数据。  

第二套方案：把数据独立出来，成为数据中心。数据只有一份，单独拿一个机房做数据中心，或者同时在各个异地多活的节点做闭环数据，可以往本地或远程读写
多节点在做子网划分的时候，除了异地多活，还可以做数据中心，所有数据在本地机房crud 闭环数据异步同步到数据中心，数据中心再去分发数据给其他机房。数据中心这里在要产生全局唯一的ID的时候，只能低效地保证一致性
（没有其他办法）

那么数据临时在本地机房不可用的时候，就可以尝试连接异地机房或数据中心。

#### 动静隔离

把静态资源从原有服务中隔离出来，参考cdn技术，走另外一套云服务，压根儿就不在我们的集群里了(小集群可以通过nginx反向代理那一套做动静分离，大集群就不行了)。**系统中尽量让服务器做少的事情、做到极致**

#### ***爬虫隔离

目前我们开发的都是API接口，并且多数都是开放的API接口。也就是说，只要有人拿到这个接口，任何人都可以通过这个API接口获取数据，那么像网络爬虫的，请求速度又快，获取的数据又多，不用多久，爬虫方完全可以用我们API的接口来开发一个同样的网站，这样的话，后果就有点严重了，所以我们需要限流，限制访问的频率

开放平台的API接口调用需要限制其频率，以节约服务器资源和避免恶意的频繁调用

在大型互联网项目中，对于web服务和网络爬虫的访问流量能达到5:1，甚至更高，有的系统有时候就会因为爬虫流量过高而导致资源耗尽，服务不可用。  

办法是限流，见下面一小节

##### 限流维度

- 登录/会话限制
- 下载限流
- 访问频率
- ip限制，黑白名单

想要分辨出来一个访问是不是爬虫，可以简单的使用nginx来分析ua处理

nginx不仅可以处理ua来分离流量，还可以通过更强大的openresty来完成更复杂的逻辑，实现一个流量网关，软防火墙。

#### 资源隔离

磁盘

数据库

